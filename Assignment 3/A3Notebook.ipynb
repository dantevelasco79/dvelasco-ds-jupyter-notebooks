{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "## 1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_%3B</th>\n",
       "      <th>char_freq_%28</th>\n",
       "      <th>char_freq_%5B</th>\n",
       "      <th>char_freq_%21</th>\n",
       "      <th>char_freq_%24</th>\n",
       "      <th>char_freq_%23</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_%3B  char_freq_%28  \\\n",
       "0             0.00            0.00  ...           0.00          0.000   \n",
       "1             0.00            0.94  ...           0.00          0.132   \n",
       "2             0.64            0.25  ...           0.01          0.143   \n",
       "3             0.31            0.63  ...           0.00          0.137   \n",
       "4             0.31            0.63  ...           0.00          0.135   \n",
       "\n",
       "   char_freq_%5B  char_freq_%21  char_freq_%24  char_freq_%23  \\\n",
       "0            0.0          0.778          0.000          0.000   \n",
       "1            0.0          0.372          0.180          0.048   \n",
       "2            0.0          0.276          0.184          0.010   \n",
       "3            0.0          0.137          0.000          0.000   \n",
       "4            0.0          0.135          0.000          0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  class  \n",
       "0                       278      1  \n",
       "1                      1028      1  \n",
       "2                      2259      1  \n",
       "3                       191      1  \n",
       "4                       191      1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_spam = pd.read_csv('spambase.csv')\n",
    "df_spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_%3B</th>\n",
       "      <th>char_freq_%28</th>\n",
       "      <th>char_freq_%5B</th>\n",
       "      <th>char_freq_%21</th>\n",
       "      <th>char_freq_%24</th>\n",
       "      <th>char_freq_%23</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>0.330885</td>\n",
       "      <td>0.712859</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.011565</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.624007</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.045247</td>\n",
       "      <td>0.045298</td>\n",
       "      <td>-0.008724</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.345359</td>\n",
       "      <td>0.051909</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.256117</td>\n",
       "      <td>0.672399</td>\n",
       "      <td>0.244743</td>\n",
       "      <td>-0.088010</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>1.086711</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.026007</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.126203</td>\n",
       "      <td>0.423783</td>\n",
       "      <td>0.008763</td>\n",
       "      <td>-0.002443</td>\n",
       "      <td>0.250563</td>\n",
       "      <td>1.228324</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.145921</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.851723</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>1.364846</td>\n",
       "      <td>0.343685</td>\n",
       "      <td>0.193644</td>\n",
       "      <td>0.036670</td>\n",
       "      <td>1.974017</td>\n",
       "      <td>0.016422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117376</td>\n",
       "      <td>0.014684</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.440053</td>\n",
       "      <td>-0.079754</td>\n",
       "      <td>0.145921</td>\n",
       "      <td>2.221106</td>\n",
       "      <td>3.258733</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.472573</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>0.500237</td>\n",
       "      <td>1.308402</td>\n",
       "      <td>0.789462</td>\n",
       "      <td>0.605857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.007511</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.161934</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.052150</td>\n",
       "      <td>-0.062466</td>\n",
       "      <td>-0.152222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.472573</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>0.500237</td>\n",
       "      <td>1.308402</td>\n",
       "      <td>0.789462</td>\n",
       "      <td>0.605857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.014910</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.164387</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.052150</td>\n",
       "      <td>-0.062466</td>\n",
       "      <td>-0.152222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0       -0.342434           0.330885       0.712859       -0.0469   \n",
       "1        0.345359           0.051909       0.435130       -0.0469   \n",
       "2       -0.145921          -0.165072       0.851723       -0.0469   \n",
       "3       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "4       -0.342434          -0.165072      -0.556761       -0.0469   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0       0.011565       -0.350266         -0.291794           -0.262562   \n",
       "1      -0.256117        0.672399          0.244743           -0.088010   \n",
       "2       1.364846        0.343685          0.193644            0.036670   \n",
       "3       0.472573       -0.350266          0.500237            1.308402   \n",
       "4       0.472573       -0.350266          0.500237            1.308402   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_%3B  char_freq_%28  \\\n",
       "0        -0.323302       -0.371364  ...      -0.158453      -0.514307   \n",
       "1        -0.323302        1.086711  ...      -0.158453      -0.026007   \n",
       "2         1.974017        0.016422  ...      -0.117376       0.014684   \n",
       "3         0.789462        0.605857  ...      -0.158453      -0.007511   \n",
       "4         0.789462        0.605857  ...      -0.158453      -0.014910   \n",
       "\n",
       "   char_freq_%5B  char_freq_%21  char_freq_%24  char_freq_%23  \\\n",
       "0      -0.155198       0.624007      -0.308355      -0.103048   \n",
       "1      -0.155198       0.126203       0.423783       0.008763   \n",
       "2      -0.155198       0.008496       0.440053      -0.079754   \n",
       "3      -0.155198      -0.161934      -0.308355      -0.103048   \n",
       "4      -0.155198      -0.164387      -0.308355      -0.103048   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                   -0.045247                    0.045298   \n",
       "1                   -0.002443                    0.250563   \n",
       "2                    0.145921                    2.221106   \n",
       "3                   -0.052150                   -0.062466   \n",
       "4                   -0.052150                   -0.062466   \n",
       "\n",
       "   capital_run_length_total  class  \n",
       "0                 -0.008724      1  \n",
       "1                  1.228324      1  \n",
       "2                  3.258733      1  \n",
       "3                 -0.152222      1  \n",
       "4                 -0.152222      1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "columns_spam = df_spam.columns.tolist()\n",
    "columns_spam.remove('class')\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "for column in columns_spam:\n",
    "    df_spam[column] = std_scaler.fit_transform(df_spam[column].values.reshape(-1,1))\n",
    "df_spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_spam = df_spam[columns_spam]\n",
    "y_spam = df_spam['class']\n",
    "\n",
    "X_spam_train,X_spam_test,y_spam_train,y_spam_test=train_test_split(X_spam,y_spam,test_size=0.25)\n",
    "\n",
    "reg_spam = LogisticRegression()\n",
    "reg_spam.fit(X_spam_train,y_spam_train)\n",
    "\n",
    "y_spam_pred=reg_spam.predict(X_spam_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[670  35]\n",
      " [ 53 393]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n",
    "cm_spam = metrics.confusion_matrix(y_spam_test, y_spam_pred)\n",
    "print(cm_spam)\n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm_spam, annot=True, fmt=\"g\", ax=ax);\n",
    "ax.set_xlabel('Predicted Label');ax.set_ylabel('Actual Label'); \n",
    "ax.set_title('Spam Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(['1', '0']); ax.yaxis.set_ticklabels(['1', '0']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 670\n",
      "False Positives: 53\n",
      "True Negatives: 393\n",
      "False Negatives: 35\n",
      "Accuracy: 0.9235447437011295\n",
      "Error: 0.07645525629887051\n",
      "Precision: 0.9266943291839558\n",
      "Recall: 0.950354609929078\n",
      "F1 Score: 0.938375350140056\n"
     ]
    }
   ],
   "source": [
    "cm_spam_tp = cm_spam[0][0]\n",
    "cm_spam_fp = cm_spam[1][0]\n",
    "cm_spam_tn = cm_spam[1][1]\n",
    "cm_spam_fn = cm_spam[0][1]\n",
    "print(\"True Positives: \" + str(cm_spam_tp) + \"\\nFalse Positives: \" + str(cm_spam_fp) + \"\\nTrue Negatives: \" + str(cm_spam_tn) + \"\\nFalse Negatives: \" + str(cm_spam_fn))\n",
    "\n",
    "accuracy_spam = (cm_spam_tp + cm_spam_tn) / (cm_spam_tp + cm_spam_tn + cm_spam_fp + cm_spam_fn)\n",
    "error_spam = 1 - accuracy_spam\n",
    "precision_spam = cm_spam_tp / (cm_spam_tp + cm_spam_fp)\n",
    "recall_spam = cm_spam_tp / (cm_spam_tp + cm_spam_fn)\n",
    "f1_score_spam = 2 * (precision_spam * recall_spam)/(precision_spam + recall_spam)\n",
    "print(\"Accuracy: \" + str(accuracy_spam) + \"\\nError: \" + str(error_spam) + \"\\nPrecision: \" + str(precision_spam) + \"\\nRecall: \" + str(recall_spam) + \"\\nF1 Score: \" + str(f1_score_spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.14581526650844068 word_freq_make\n",
      "-0.1822091419066656 word_freq_address\n",
      "0.05640481078568639 word_freq_all\n",
      "0.8083260393679136 word_freq_3d\n",
      "0.3251380383105797 word_freq_our\n",
      "0.14300612626768094 word_freq_over\n",
      "0.7877867094154879 word_freq_remove\n",
      "0.24145686724997553 word_freq_internet\n",
      "0.3058938700176436 word_freq_order\n",
      "0.0569650118244484 word_freq_mail\n",
      "-0.0024995856954366544 word_freq_receive\n",
      "-0.10772606615668938 word_freq_will\n",
      "0.0033553776468833213 word_freq_people\n",
      "0.07266732675207742 word_freq_report\n",
      "0.1882811116058336 word_freq_addresses\n",
      "0.8622749573705061 word_freq_free\n",
      "0.4907160023617372 word_freq_business\n",
      "0.17341396977565104 word_freq_email\n",
      "0.1754334937155807 word_freq_you\n",
      "0.3874909950429233 word_freq_credit\n",
      "0.34194158260715674 word_freq_your\n",
      "0.26241353796883393 word_freq_font\n",
      "0.8053009530253088 word_freq_000\n",
      "0.1782243270597861 word_freq_money\n",
      "-2.298870197569407 word_freq_hp\n",
      "-1.028921503275913 word_freq_hpl\n",
      "-4.002047371281401 word_freq_george\n",
      "0.24394969550157783 word_freq_650\n",
      "-0.8539363382512252 word_freq_lab\n",
      "-0.16746294493191868 word_freq_labs\n",
      "-0.3343765360264746 word_freq_telnet\n",
      "0.35022448980417653 word_freq_857\n",
      "-0.7954367819206333 word_freq_data\n",
      "-1.2130497986848703 word_freq_415\n",
      "-0.9528466539767785 word_freq_85\n",
      "0.32289355907525996 word_freq_technology\n",
      "0.024848592679367888 word_freq_1999\n",
      "-0.1428279431906059 word_freq_parts\n",
      "-0.3915232223726279 word_freq_pm\n",
      "-0.20287406159182397 word_freq_direct\n",
      "-1.6482581078668785 word_freq_cs\n",
      "-1.519260900389041 word_freq_meeting\n",
      "-0.2065351177090401 word_freq_original\n",
      "-0.7778916490235563 word_freq_project\n",
      "-0.6778965227676658 word_freq_re\n",
      "-1.0193040455112818 word_freq_edu\n",
      "-0.17542311173134467 word_freq_table\n",
      "-0.7031303400377541 word_freq_conference\n",
      "-0.31432028983844545 char_freq_%3B\n",
      "-0.06588686549791405 char_freq_%28\n",
      "-0.061004807318902156 char_freq_%5B\n",
      "0.636464338537747 char_freq_%21\n",
      "1.1230932088583947 char_freq_%24\n",
      "0.9018238802559418 char_freq_%23\n",
      "-0.17621853245267863 capital_run_length_average\n",
      "0.8234226072288342 capital_run_length_longest\n",
      "0.5089376199341608 capital_run_length_total\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(columns_spam)):\n",
    "    print(reg_spam.coef_[0][idx], columns_spam[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The features that contribute the most to the prediction which are negatively correlated with the spam class are the frequencies of george, hp, lab, telnet, 85, cs, meeting, edu. Those that contribute the most that are positively correlated with spam are the frequencies of 3d, remove, and %24 as well as capital_run_length_longest and capital_run_length_average.\n",
    "\n",
    "## 1c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.8992180712423979, PREC:0.9696969696969697, REC:0.8624113475177305 for Threshold 0.25\n",
      "ACC:0.9235447437011295, PREC:0.9266943291839558, REC:0.950354609929078 for Threshold 0.5\n",
      "ACC:0.9000868809730669, PREC:0.8734177215189873, REC:0.9787234042553191 for Threshold 0.75\n",
      "ACC:0.8531711555169418, PREC:0.8130841121495327, REC:0.9872340425531915 for Threshold 0.9\n"
     ]
    }
   ],
   "source": [
    "prob_results_spam = reg_spam.predict_proba(X_spam_test)\n",
    "thresholds = [.25, .5, .75, .9]\n",
    "for t in thresholds:\n",
    "    y_spam_pred_t = []\n",
    "    for prob in prob_results_spam:\n",
    "        if (prob[1] > t):\n",
    "            y_spam_pred_t.append(1)\n",
    "        else:\n",
    "            y_spam_pred_t.append(0)\n",
    "    cm_spam_t = metrics.confusion_matrix(y_spam_test, y_spam_pred_t)\n",
    "    accuracy_spam_t = (cm_spam_t[0][0] + cm_spam_t[1][1]) / (cm_spam_t[0][0] + cm_spam_t[1][1] + cm_spam_t[1][0] + cm_spam_t[0][1])\n",
    "    precision_spam_t = cm_spam_t[0][0] / (cm_spam_t[0][0] + cm_spam_t[1][0])\n",
    "    recall_spam_t = cm_spam_t[0][0] / (cm_spam_t[0][0] + cm_spam_t[0][1])\n",
    "    print(\"ACC:\" + str(accuracy_spam_t) + \", PREC:\" + str(precision_spam_t) + \", REC:\" + str(recall_spam_t) + \" for Threshold \" + str(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from math import e\n",
    "import random as rd\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def neg_log_MLE(y_real_values, y_pred_values):\n",
    "    sum = 0\n",
    "    for idx in range(len(y_real_values)):\n",
    "        num = y_pred_values[idx]\n",
    "        sum -= (y_real_values[idx] * math.log(num) + (1 - y_real_values[idx]) * (1 - math.log(num)))\n",
    "    return sum\n",
    "\n",
    "def determine_class(y_pred_values):\n",
    "    pred = []\n",
    "    for idx in range(len(y_pred_values)):\n",
    "        if (y_pred_values[idx] > .5):\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            pred.append(0)\n",
    "    pred = np.array(pred)\n",
    "    return pred\n",
    "\n",
    "def pred(x_values, theta):\n",
    "    pred = []\n",
    "    for idx in range(len(x_values)):\n",
    "        num = 1 / (1 + e ** -(np.dot(theta, x_values[idx])))\n",
    "        pred.append(num)\n",
    "    pred = np.array(pred)\n",
    "    return pred\n",
    "\n",
    "def sum_grad(X, y, theta, column):\n",
    "    sum = 0\n",
    "    for idx in range(len(X.values)):\n",
    "        sum += ((1 / (1 + e ** -np.dot(theta, X.values[idx]))) - y.values[idx]) * X.values[idx][column]\n",
    "    return sum\n",
    "                \n",
    "def grad_desc(alpha, max_iter, init_range, min_change, X, y):\n",
    "    iter = 0\n",
    "    theta = []\n",
    "    for idx in range(len(X.values[0])):\n",
    "        theta.append(rd.randint(-init_range, init_range))\n",
    "    theta = np.array(theta)\n",
    "    min_theta = theta\n",
    "    while(iter < max_iter):\n",
    "        iter += 1\n",
    "        change = []\n",
    "        for idx in range(len(theta)):\n",
    "            change.append(alpha * sum_grad(X, y, theta, idx))\n",
    "        change = np.array(change)\n",
    "        if (LA.norm(theta - change - theta) < min_change):\n",
    "            if (neg_log_MLE(y.values, pred(X.values, theta)) < neg_log_MLE(y.values, pred(X.values, min_theta))):\n",
    "                min_theta = theta\n",
    "            for idx in range(len(X.values[0])):\n",
    "                theta[idx] = rd.randint(-init_range, init_range)\n",
    "        else:\n",
    "            theta = theta - change\n",
    "    if (neg_log_MLE(y.values, pred(X.values, theta)) < neg_log_MLE(y.values, pred(X.values, min_theta))):\n",
    "        min_theta = theta\n",
    "    return min_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'alpha': [], 'max_iterations': [], 'loss_obj': [], 'accuracy': [], 'precision': [], 'recall': [], 'score': []}\n",
    "alphas = [0.01, 0.1, 0.5]\n",
    "iters = [10, 50, 100]\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        \n",
    "        dict['alpha'].append(alphas[i])\n",
    "        dict['max_iterations'].append(iters[j])\n",
    "        \n",
    "        theta_spam_grad = grad_desc(alphas[i], iters[j], 10, .01, X_spam, y_spam)\n",
    "        \n",
    "        y_spam_pred_grad = pred(X_spam.values, theta_spam_grad) \n",
    "\n",
    "        loss_obj = neg_log_MLE(y_spam.values, y_spam_pred_grad)\n",
    "        dict['loss_obj'].append(loss_obj)\n",
    "        \n",
    "        y_spam_pred_grad = determine_class(y_spam_pred_grad)\n",
    "        \n",
    "        if (iters[j] == 100):\n",
    "            cm_spam_grad = metrics.confusion_matrix(y_spam, y_spam_pred_grad)\n",
    "            accuracy_spam_grad = (cm_spam_grad[0][0] + cm_spam_grad[1][1]) / (cm_spam_grad[0][0] + cm_spam_grad[1][1] + cm_spam_grad[1][0] + cm_spam_grad[0][1])\n",
    "            precision_spam_grad = cm_spam_grad[0][0] / (cm_spam_grad[0][0] + cm_spam_grad[1][0])\n",
    "            recall_spam_grad = cm_spam_grad[0][0] / (cm_spam_grad[0][0] + cm_spam_grad[0][1])\n",
    "            f1_score_spam_grad = 2 * (precision_spam_grad * recall_spam_grad)/(precision_spam_grad + recall_spam_grad)\n",
    "            dict['accuracy'].append(accuracy_spam_grad)\n",
    "            dict['precision'].append(precision_spam_grad)\n",
    "            dict['recall'].append(recall_spam_grad)\n",
    "            dict['score'].append(f1_score_spam_grad)\n",
    "        else:\n",
    "            dict['accuracy'].append(-1)\n",
    "            dict['precision'].append(-1)\n",
    "            dict['recall'].append(-1)\n",
    "            dict['score'].append(-1)\n",
    "grad_spam_df = pd.DataFrame(dict)\n",
    "grad_spam_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "## 3a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k = 1, ACC:0.5305821025195482, ERR:0.4694178974804518, PREC:0.610754189944134, REC:0.6255418822624269\n",
      "For k = 2, ACC:0.5393570807993051, ERR:0.460642919200695, PREC:0.6083547557840616, REC:0.6772886857946107\n",
      "For k = 3, ACC:0.5288444830582102, ERR:0.4711555169417898, PREC:0.6092050209205021, REC:0.6248989887808072\n",
      "For k = 4, ACC:0.5431798436142484, ERR:0.4568201563857516, PREC:0.6111256544502617, REC:0.6712367499152305\n",
      "For k = 5, ACC:0.5218940052128583, ERR:0.4781059947871416, PREC:0.6034626038781163, REC:0.6226896702565711\n",
      "For k = 6, ACC:0.530668983492615, ERR:0.4693310165073849, PREC:0.607343124165554, REC:0.648958637237534\n",
      "For k = 7, ACC:0.5373588184187662, ERR:0.4626411815812338, PREC:0.6161865569272977, REC:0.6399671197405559\n",
      "For k = 8, ACC:0.5278019113814075, ERR:0.47219808861859247, PREC:0.5961281708945261, REC:0.6495814418695225\n",
      "For k = 9, ACC:0.5344917463075587, ERR:0.46550825369244136, PREC:0.6054421768707483, REC:0.644136228694575\n",
      "For k = 10, ACC:0.5296264118158124, ERR:0.4703735881841876, PREC:0.6074666666666666, REC:0.6484151740686228\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def knn_pred_k(k, X_train, y_train, X_test, y_test):\n",
    "    knn_spam = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_spam.fit(X_spam_train, y_spam_train)\n",
    "    pred = knn_spam.predict(X_spam_test)\n",
    "    return pred\n",
    "\n",
    "for idx in range(10):\n",
    "    accuracy_spam_k = 0\n",
    "    error_spam_k = 0\n",
    "    precision_spam_k = 0\n",
    "    recall_spam_k = 0\n",
    "    for jdx in range(10):\n",
    "        X_spam_train_knn,X_spam_test_knn,y_spam_train_knn,y_spam_test_knn=train_test_split(X_spam,y_spam,test_size=0.25)\n",
    "        pred = knn_pred_k(idx + 1, X_spam_train_knn, y_spam_train_knn, X_spam_test_knn, y_spam_test_knn)\n",
    "        cm_spam_k = metrics.confusion_matrix(y_spam_test_knn, pred)\n",
    "        accuracy_spam_k += (cm_spam_k[0][0] + cm_spam_k[1][1]) / (cm_spam_k[0][0] + cm_spam_k[1][1] + cm_spam_k[1][0] + cm_spam_k[0][1])\n",
    "        error_spam_k += (cm_spam_k[1][0] + cm_spam_k[0][1]) / (cm_spam_k[0][0] + cm_spam_k[1][1] + cm_spam_k[1][0] + cm_spam_k[0][1])\n",
    "        precision_spam_k += cm_spam_k[0][0] / (cm_spam_k[0][0] + cm_spam_k[1][0])\n",
    "        recall_spam_k += cm_spam_k[0][0] / (cm_spam_k[0][0] + cm_spam_k[0][1])\n",
    "    accuracy_spam_k /= 10\n",
    "    error_spam_k /= 10\n",
    "    precision_spam_k /= 10\n",
    "    recall_spam_k /= 10\n",
    "    print(\"For k = \" + str(idx + 1) + \", ACC:\" + str(accuracy_spam_k) + \", ERR:\" + str(error_spam_k) + \", PREC:\" + str(precision_spam_k) + \", REC:\" + str(recall_spam_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I will use k = 8\n",
    "\n",
    "## 3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_k = knn_pred_k(8, X_spam_train, y_spam_train, X_spam_test, y_spam_test)\n",
    "cm_spam_k = metrics.confusion_matrix(y_spam_test, pred_k)\n",
    "accuracy_spam_k = (cm_spam_k[0][0] + cm_spam_k[1][1]) / (cm_spam_k[0][0] + cm_spam_k[1][1] + cm_spam_k[1][0] + cm_spam_k[0][1])\n",
    "error_spam_k = 1 - accuracy_spam_k\n",
    "precision_spam_k = cm_spam_k[0][0] / (cm_spam_k[0][0] + cm_spam_k[1][0])\n",
    "recall_spam_k = cm_spam_k[0][0] / (cm_spam_k[0][0] + cm_spam_k[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda_spam = LinearDiscriminantAnalysis()\n",
    "lda_spam.fit(X_spam_train,y_spam_train)\n",
    "\n",
    "y_spam_pred_lda =lda_spam.predict(X_spam_test)\n",
    "cm_spam_lda = metrics.confusion_matrix(y_spam_test, y_spam_pred_lda)\n",
    "accuracy_spam_lda = (cm_spam_lda[0][0] + cm_spam_lda[1][1]) / (cm_spam_lda[0][0] + cm_spam_lda[1][1] + cm_spam_lda[1][0] + cm_spam_lda[0][1])\n",
    "error_spam_lda = 1 - accuracy_spam_lda\n",
    "precision_spam_lda = cm_spam_lda[0][0] / (cm_spam_lda[0][0] + cm_spam_lda[1][0])\n",
    "recall_spam_lda = cm_spam_lda[0][0] / (cm_spam_lda[0][0] + cm_spam_lda[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For logistic regression, ACC:0.9235447437011295, ERR:0.07645525629887051, PREC:0.9266943291839558, REC:0.950354609929078\n",
      "For kNN, ACC:0.9044309296264118, ERR:0.0955690703735882, PREC:0.897196261682243, REC:0.9531914893617022\n",
      "For LDA, ACC:0.8879235447437012, ERR:0.11207645525629883, PREC:0.8779527559055118, REC:0.948936170212766\n"
     ]
    }
   ],
   "source": [
    "print(\"For logistic regression, ACC:\" + str(accuracy_spam) + \", ERR:\" + str(error_spam) + \", PREC:\" + str(precision_spam) + \", REC:\" + str(recall_spam))\n",
    "print(\"For kNN, ACC:\" + str(accuracy_spam_k) + \", ERR:\" + str(error_spam_k) + \", PREC:\" + str(precision_spam_k) + \", REC:\" + str(recall_spam_k))\n",
    "print(\"For LDA, ACC:\" + str(accuracy_spam_lda) + \", ERR:\" + str(error_spam_lda) + \", PREC:\" + str(precision_spam_lda) + \", REC:\" + str(recall_spam_lda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression performs best while LDA performs worst. My guess as to why LDA is performing worse is that it is a generative model and there are only two classes.\n",
    "\n",
    "## 3c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, metrics, model_selection, svm\n",
    "\n",
    "metrics.plot_roc_curve(reg_spam, X_spam_test, y_spam_test) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1630fa43b88>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAdJUlEQVR4nO3dfZRddX3v8ffnzEMG8sBTgkHyiAYkoAIdUeoTXSACVvC6UKHXq1aU23rRtlpusb1XWtoub9VKr5ZeG9EFYgVRVzW2WFoQBbmABAgohNQYSDIXSiYhxDyQc+ac871/nH0yZ86cmUyS2ecw8/u81pqVs/f+zdnfPZP5fs/v99sPigjMzCxdhU4HYGZmneVCYGaWOBcCM7PEuRCYmSXOhcDMLHEuBGZmiXMhMDNLnAuBTSuSnpL0gqSdkv5D0vWSZjW1+XVJP5S0Q9J2Sd+XtLypzRxJfyNpY/Ze67LluWPsV5I+JunnknZJGpD0LUmvzPN4zSaDC4FNR2+PiFnAKcCpwCfrGySdAfwr8D3gpcBS4BHgHknHZW16gTuAk4BzgTnArwNbgdPH2Of/Bn4P+BhwJHA88F3gbfsbvKTu/f0es4MhX1ls04mkp4APRcTt2fJngJMi4m3Z8t3AzyLiI03f9wNgMCLeJ+lDwF8CL4uInRPY5zLgCeCMiPjpGG1+BHw9Iq7Llj+QxfmGbDmAy4HfB7qB24CdEfGHDe/xPeDHEfF5SS8Fvgi8CdgJXBMRX5jAj8hsFPcIbNqStAA4D1iXLR9K7ZP9t1o0vwV4S/b6bOBfJlIEMmcBA2MVgf3wDuC1wHLgG8B7JAlA0hHAOcDNkgrA96n1ZI7N9v/7kt56kPu3RLkQ2HT0XUk7gE3AZuCqbP2R1P7PP9Pie54B6uP/R43RZiz7234sn46I5yLiBeBuIIA3ZtsuAu6NiKeB1wDzIuLqiChFxHrgy8DFkxCDJciFwKajd0TEbOBM4BUMJ/htQBU4psX3HANsyV5vHaPNWPa3/Vg21V9Ebcz2ZuCSbNVvAf+QvV4MvFTS8/Uv4I+Bl0xCDJYgFwKbtiLix8D1wOey5V3AvcC7WjR/N7UJYoDbgbdKmjnBXd0BLJDUP06bXcChDcvzW4XctHwTcJGkxdSGjL6Trd8EPBkRhzd8zY6I8ycYr9kILgQ23f0N8BZJp2TLVwLvz071nC3pCEl/AZwB/FnW5kZqyfY7kl4hqSDpKEl/LGlUso2IXwB/B9wk6UxJvZL6JF0s6cqs2WrgnZIOlfRy4NJ9BR4RDwODwHXAbRHxfLbpp8CvJP2RpEMkdUk6WdJrDuQHZOZCYNNaRAwCXwP+Z7b8E+CtwDupjetvoHaK6RuyhE5EFKlNGD8B/BvwK2rJdy5w/xi7+hjwt8C1wPPAL4H/RG1SF+AaoAQ8C9zA8DDPvtyUxfKNhmOqAG+ndnrsk9SGtK4DDpvge5qN4NNHzcwS5x6BmVniXAjMzBLnQmBmljgXAjOzxE25m1vNnTs3lixZ0ukwzMymlAcffHBLRMxrtW3KFYIlS5awatWqTodhZjalSNow1jYPDZmZJc6FwMwscS4EZmaJcyEwM0ucC4GZWeJyKwSSvipps6Sfj7Fdkr6QPRT8UUmn5RWLmZmNLc8ewfXUHvw9lvOAZdnXZcD/yTEWMzMbQ27XEUTEXZKWjNPkQuBr2ZOY7pN0uKRjImIyHvlnZrbfIoJKNShXg2r2ulqFcrVKJYZfV6tQiaBSrVKpQqVaa1upf08E5crwezRvrzS8f7kaVJu2NbavNrQ568SX8OqFh0/6cXfygrJjaXg0HzCQrRtVCCRdRq3XwKJFi9oSnNlUV21KaJUIKpWRyWVv0mpIRo0JrJ7QRiTBiCz5ZUmwISHufY/6+7VIgKP22SJBlpviHJ1Q68c3nJRb7bP5OPcVz4v9rvxHz+mbdoVALda1/DVExApgBUB/f/+L/Fdlky2i8Q+7+RPZ2H/0jQlt5Ce8rF3Tp619Javh9xgjCbb69NecePdnn60+VWY/g+ZjapXgXswk6C6IgkRXoeGrYbkg0d1VW1do3lYQ3YX6NugtdGVtoKtQoKvA8HsUhr+/u6tpn037HzeeEfsc+b61/dX2W9tngUKB1vtsfo9s24j362qKLWufl04WggFgYcPyAuDpDsViExARlCpV9gxVKQ5Vav+Wa//uKVfYM1ShuPd1lT1D2bpy1r48ct2eoeF29eViU5tiufqiT2pdDQmpu1CgoGxdPSE1/GE3J7TmP/SergJ9PaMTWHOy2rtt7z5HJqPWyWrsZDQqCbbaZ1dT0trbJttv48+gngTrr5veV8ovqdn+62QhWAlcLulmag/m3u75gclXrQY7S2V27Cmzc0+ZHXuG2FEcubwzW96xp8zO4lD2b235hVJlOMmXqwfVde7rKdDX00Vfdxczegr0dXfR11NgRk8Xcw7pYd7sGdn2WrsZ3QVm9BToKhRGJKvh5AZdXYWWn8hGJ6uxP+GN12Z04h35PQXhpGZTXm6FQNJNwJnAXEkDwFVAD0BEfAm4FTgfWAfsBn47r1imu1/tGeLHawf54RObeWrrrizB15L5zmJ5n99fEMya0c3svh5m93Uza0Y3R83sZfFRMzm0J0vaWYKekSXovp6u7KswnNizJN/XU2BGQ5Lv6ynQ21VwwjR7kcrzrKFL9rE9gP+W1/6nu41bd3P7mme544lnuX/9c5SrwZEze1l+zBzmz+nLEnotsde/6suz+rqZ07B8aG+Xk7RZwqbcbahTVakGqzc9zx1rnuX2Nc/y78/uBGDZ0bP48JuO4+wTj+aUhUfQleOEkplNTy4EL2K7S2Xu/sUWbn/8We5cu5ktO0t0FcRrlx7Je16ziLNPPJrFR83sdJhmNsW5ELwIlMpVNj63mye37GL94M7s312sHnieUrnKnL5uzjzhaM5e/hLefPw8Djukp9Mhm9k04kLQARHBXb/Ywo33PsUvNu9k03O7aTxDcu6sXpbOncl/ed1izjrxaF6z5Eh6unx/QDPLhwtBmz2y6Xn+6l+e4P/+civHHNbHaYuP4IJXv5Tj5s1k6dxZLJ0705/4zaytXAja5Mktu/jcv67lnx99hiNn9vKnb1/Ob712Mb3d/qRvZp3lQpCzzTv28MU71nHTTzfS213gY2ct48NvXMrsPn/qN7MXBxeCnOwslllx13quu3s9pXKVS05fxEfPejlHz+7rdGhmZiO4EEyyUrnKN+7fwBd/uI6tu0q87VXH8IfnnMDSuT7N08xenFwIJtFjT2/nI//wEBu27uaM447iyvNekcstY83MJpMLwSS5Z90W/uuNDzKnr5sbPng6b1o217dtMLMpwYVgEqx85Gk+cctqjps7i+s/+BqOOeyQTodkZjZhLgQH6bq71/MX/7yG05ceyZff1+9rAMxsynEhOEDVavDpH6zhy3c/yXknz+ea95xCX09Xp8MyM9tvLgQHoFSucsW3H+F7q5/mfWcs5qq3n+S7fprZlOVCsJ92Fsv8zo0P8pN1W7jirSfwkTNf5klhM5vSXAj2Q6UafOiGB3jgqW189qJX8a7+hfv+JjOzFznf6GY/rLhrPfetf45Pv/OVLgJmNm24EEzQz//fdj7/b2s5/5XzedevLeh0OGZmk8aFYAJeKFX4vZsf5siZvfzlO17pOQEzm1Y8RzABn/7BGn45uIuvX/pajpjZ2+lwzMwmlXsE+3DnE5v52r0buPQNS3nDsrmdDsfMbNK5EIxjy84iV3z7UV4xfzZXvPWETodjZpYLDw2NISK48js/41cvDPH1D53uq4bNbNpyj2AMNz+widvXPMt/P/cEXjF/TqfDMTPLjQtBC+sHd3L19x/nDS+fywdfv7TT4ZiZ5cqFoEm5UuUPvrma3u4Cn3vXqyn4HkJmNs25EDS555dbeWRgO1e9fTnzD/Pzhc1s+nMhaPLghm0UBOecNL/ToZiZtYULQZOHNmzjhPlzmDXDJ1SZWRpcCBpUqsHqTc9z2iI/cN7M0pFrIZB0rqS1ktZJurLF9kWS7pT0sKRHJZ2fZzz78ovNO9hZLHPaoiM6GYaZWVvlVggkdQHXAucBy4FLJC1vavY/gFsi4lTgYuDv8opnIh7a8DwApy12ITCzdOTZIzgdWBcR6yOiBNwMXNjUJoD61VqHAU/nGM8+PbRxG0fO7GXJUYd2Mgwzs7bKsxAcC2xqWB7I1jX6U+C9kgaAW4GPtnojSZdJWiVp1eDgYB6xArWJ4tMWHe7bTJtZUvIsBK2yaTQtXwJcHxELgPOBGyWNiikiVkREf0T0z5s3L4dQYduuEuu37OJUzw+YWWLyLAQDQOPzHBcweujnUuAWgIi4F+gDOnKv54c3bQPwRLGZJSfPQvAAsEzSUkm91CaDVza12QicBSDpRGqFIL+xn3E8tOF5ugri1QsP68Tuzcw6JrdCEBFl4HLgNmANtbODHpN0taQLsmafAD4s6RHgJuADEdE8fNQWD23cxonHzObQXl9IZmZpyTXrRcSt1CaBG9d9quH148Dr84xhIsqVKqs3Pc9Ffii9mSXIVxYDa5/dwe5SxfMDZpYkFwLgoY3ZhWQuBGaWIBcC4OEN25g7q5eFRx7S6VDMzNrOhYDaRPFpi47whWRmlqTkC8GWnUWe2rrb9xcys2QlXwge9vyAmSUu+ULw0MZtdBfEqxb4QjIzS5MLwYZtnPTSOfT1dHU6FDOzjki6EAxVqjw6sN03mjOzpCVdCJ54ZgcvDFU8UWxmSUu6EDy0sX7HUT+j2MzSlXwhOHr2DI493BeSmVm6ki8Ev7bYF5KZWdqSLQSbd+xh03Mv+PoBM0tesoVgdf1CssWeHzCztCVbCLbtLgEw/zDPD5hZ2pItBMVyFYAZ3cn+CMzMgIQLQSkrBL0uBGaWuGSzoHsEZmY1yWbB4lAFgN6uZH8EZmZAyoWgUqW3u+BrCMwseekWgqGqh4XMzEi5EJRdCMzMIOFCUCpXmdHtZxCYmSVbCIrlinsEZmYkXQiqvobAzIyEC0HJcwRmZkDChaA2NOQ5AjOzhAuBh4bMzCDhQuChITOzmlwzoaRzJa2VtE7SlWO0ebekxyU9JukbecbTqFiuMqPHhcDMrDuvN5bUBVwLvAUYAB6QtDIiHm9oswz4JPD6iNgm6ei84mlWKld9nyEzM/LtEZwOrIuI9RFRAm4GLmxq82Hg2ojYBhARm3OMZwRPFpuZ1eRZCI4FNjUsD2TrGh0PHC/pHkn3STq31RtJukzSKkmrBgcHJyU4Dw2ZmdXkmQlb3dYzmpa7gWXAmcAlwHWSRj1EOCJWRER/RPTPmzdvUoLz0JCZWU2emXAAWNiwvAB4ukWb70XEUEQ8CaylVhhy5x6BmVlNnpnwAWCZpKWSeoGLgZVNbb4L/AaApLnUhorW5xgTAOVKlUo1PEdgZkaOhSAiysDlwG3AGuCWiHhM0tWSLsia3QZslfQ4cCdwRURszSumulLFzys2M6vL7fRRgIi4Fbi1ad2nGl4H8PHsq22KQ35esZlZXZKZsP7gevcIzMwSLQSlcr1H4DkCM7MkC0GxXAE8NGRmBskWAg8NmZnVJZkJi2VPFpuZ1SWZCYeHhjxHYGa234VAUpek/5xHMO3ioSEzs2FjZkJJcyR9UtLfSjpHNR+lduXvu9sX4uQreWjIzGyv8S4ouxHYBtwLfAi4AugFLoyI1W2ILTf1HkGf7zVkZjZuITguIl4JIOk6YAuwKCJ2tCWyHNV7BL1dniMwMxvvI/FQ/UVEVIAnp0MRgIbJYvcIzMzG7RG8WtKvGH6uwCENyxERc3KPLie+15CZ2bAxC0FETNtxE9991Mxs2JiFQFIf8DvAy4FHga9mt5ae8uo9Aj+hzMxs/DmCG4B+4GfA+cBftyWiNiiWK3QXRLcLgZnZuHMEyxvOGvoK8NP2hJS/UrnqYSEzs8xEzxqaFkNCdcVy1RPFZmaZ8XoEp2RnCUHtTKHpc9ZQueL7DJmZZcYrBI9ExKlti6SNPDRkZjZsvGwYbYuizTw0ZGY2bLwewdGSxnyofER8Pod42qLoHoGZ2V7jFYIuYBbDVxZPGyX3CMzM9hqvEDwTEVe3LZI28mSxmdmw8T4WT7ueQJ0ni83Mho2XDc9qWxRt5sliM7NhY2bDiHiunYG0U7FcZUaPh4bMzCDRh9eXylXfcM7MLJNkNiyWK34ojZlZJslsWBzyHIGZWV2S2bBY8VlDZmZ1yWXDiMguKPNksZkZ5FwIJJ0raa2kdZKuHKfdRZJCUn+e8UDtjCHw84rNzOpyy4aSuoBrgfOA5cAlkpa3aDcb+Bhwf16xNKo/r9iFwMysJs9seDqwLiLWR0QJuBm4sEW7Pwc+A+zJMZa96s8rdiEwM6vJMxseC2xqWB7I1u0l6VRgYUT803hvJOkySaskrRocHDyooIrlCoDnCMzMMnkWglb3Ktr7jANJBeAa4BP7eqOIWBER/RHRP2/evIMKqpTNEfisITOzmjyz4QCwsGF5AfB0w/Js4GTgR5KeAl4HrMx7wtiTxWZmI+WZDR8AlklaKqkXuBhYWd8YEdsjYm5ELImIJcB9wAURsSrHmIYLga8sNjMDciwEEVEGLgduA9YAt0TEY5KulnRBXvvdl71DQ12eIzAzg/EfTHPQIuJW4NamdZ8ao+2ZecZSt3ey2D0CMzMgwSuLh3sEyR26mVlLyWVDzxGYmY2UXDb0dQRmZiMlVwh8HYGZ2UjJZUNfR2BmNlJy2dD3GjIzGym5bFi/+6iHhszMapLLhsWh2mSxTx81M6tJLhsWy7XnFUut7olnZpaeJAuBh4XMzIYllxGLfl6xmdkICRaCis8YMjNrkFxGLGVzBGZmVpNcRvQcgZnZSMllxFK5yowezxGYmdUlVwiK5QozfA2BmdleyWXEYrnqW1CbmTVILiN6stjMbKTkMqIni83MRkouI9auI/BksZlZXXKFoFSu+oZzZmYNksuIniw2MxspuYxYHPJksZlZo+QyYqniyWIzs0ZJZcRypUqlGp4sNjNrkFQh8IPrzcxGSyojlsp+XrGZWbOkMuJwj8BDQ2ZmdYkVgtqD6z00ZGY2LKmM6KEhM7PRcs2Iks6VtFbSOklXttj+cUmPS3pU0h2SFucZjyeLzcxGyy0jSuoCrgXOA5YDl0ha3tTsYaA/Il4FfBv4TF7xQEMh8INpzMz2yvOj8enAuohYHxEl4GbgwsYGEXFnROzOFu8DFuQYz945At9ryMxsWJ4Z8VhgU8PyQLZuLJcCP2i1QdJlklZJWjU4OHjAAQ33CFwIzMzq8syIarEuWjaU3gv0A59ttT0iVkREf0T0z5s374ADKnmOwMxslO4c33sAWNiwvAB4urmRpLOBPwHeHBHFHOPxZLGZWQt5ZsQHgGWSlkrqBS4GVjY2kHQq8PfABRGxOcdYACgO1a8j8GSxmVldboUgIsrA5cBtwBrgloh4TNLVki7Imn0WmAV8S9JqSSvHeLtJUaq4R2Bm1izPoSEi4lbg1qZ1n2p4fXae+29WHPIFZWZmzZLKiL7XkJnZaEkVAt9iwsxstKQyYrFcobsgugqtzmw1M0tTYoXAzys2M2uWVFYslf28YjOzZkllxWK54oliM7MmSRWCUrnq+wyZmTVJKisWy1XfedTMrElSWbHoHoGZ2ShJZcVSueo5AjOzJkkVgmK54qEhM7MmSWVFDw2ZmY2WVFYs+YIyM7NRksqKxXKVXs8RmJmNkFYhGKq4R2Bm1iSprFiqeGjIzKxZUlmxOOR7DZmZNUsqKxZ9HYGZ2SjJFIKIoFRxj8DMrFkyWXH4MZXJHLKZ2YQkkxVdCMzMWksmK5ZcCMzMWkomKxbLFQBPFpuZNUmmEOztEfheQ2ZmIySTFetzBL77qJnZSMlkxaJ7BGZmLSWTFYcniz1HYGbWKJlCUJ8s9gVlZmYjJZMVi0M+fdTMrJVksmKp4qEhM7NWkikEHhoyM2st16wo6VxJayWtk3Rli+0zJH0z236/pCV5xeKhITOz1nLLipK6gGuB84DlwCWSljc1uxTYFhEvB64B/iqveIaHhlwIzMwa5ZkVTwfWRcT6iCgBNwMXNrW5ELghe/1t4CxJyiOYeo/AQ0NmZiPlmRWPBTY1LA9k61q2iYgysB04qvmNJF0maZWkVYODgwcUzOKjDuW8k+d7stjMrEl3ju/d6pN9HEAbImIFsAKgv79/1PaJOOek+Zxz0vwD+VYzs2ktzx7BALCwYXkB8PRYbSR1A4cBz+UYk5mZNcmzEDwALJO0VFIvcDGwsqnNSuD92euLgB9GxAF94jczswOT29BQRJQlXQ7cBnQBX42IxyRdDayKiJXAV4AbJa2j1hO4OK94zMystTznCIiIW4Fbm9Z9quH1HuBdecZgZmbj87mUZmaJcyEwM0ucC4GZWeJcCMzMEqepdrampEFgwwF++1xgyySGMxX4mNPgY07DwRzz4oiY12rDlCsEB0PSqojo73Qc7eRjToOPOQ15HbOHhszMEudCYGaWuNQKwYpOB9ABPuY0+JjTkMsxJzVHYGZmo6XWIzAzsyYuBGZmiZuWhUDSuZLWSlon6coW22dI+ma2/X5JS9of5eSawDF/XNLjkh6VdIekxZ2IczLt65gb2l0kKSRN+VMNJ3LMkt6d/a4fk/SNdsc42Sbwf3uRpDslPZz9/z6/E3FOFklflbRZ0s/H2C5JX8h+Ho9KOu2gdxoR0+qL2i2vfwkcB/QCjwDLm9p8BPhS9vpi4JudjrsNx/wbwKHZ699N4ZizdrOBu4D7gP5Ox92G3/My4GHgiGz56E7H3YZjXgH8bvZ6OfBUp+M+yGN+E3Aa8PMxtp8P/IDaEx5fB9x/sPucjj2C04F1EbE+IkrAzcCFTW0uBG7IXn8bOEtSq8dmThX7POaIuDMidmeL91F7YtxUNpHfM8CfA58B9rQzuJxM5Jg/DFwbEdsAImJzm2OcbBM55gDmZK8PY/STEKeUiLiL8Z/UeCHwtai5Dzhc0jEHs8/pWAiOBTY1LA9k61q2iYgysB04qi3R5WMix9zoUmqfKKayfR6zpFOBhRHxT+0MLEcT+T0fDxwv6R5J90k6t23R5WMix/ynwHslDVB7/slH2xNax+zv3/s+5fpgmg5p9cm++RzZibSZSiZ8PJLeC/QDb841ovyNe8ySCsA1wAfaFVAbTOT33E1teOhMar2+uyWdHBHP5xxbXiZyzJcA10fEX0s6g9pTD0+OiGr+4XXEpOev6dgjGAAWNiwvYHRXcW8bSd3UupPjdcVe7CZyzEg6G/gT4IKIKLYptrzs65hnAycDP5L0FLWx1JVTfMJ4ov+3vxcRQxHxJLCWWmGYqiZyzJcCtwBExL1AH7Wbs01XE/p73x/TsRA8ACyTtFRSL7XJ4JVNbVYC789eXwT8MLJZmClqn8ecDZP8PbUiMNXHjWEfxxwR2yNibkQsiYgl1OZFLoiIVZ0Jd1JM5P/2d6mdGICkudSGita3NcrJNZFj3gicBSDpRGqFYLCtUbbXSuB92dlDrwO2R8QzB/OG025oKCLKki4HbqN2xsFXI+IxSVcDqyJiJfAVat3HddR6Ahd3LuKDN8Fj/iwwC/hWNi++MSIu6FjQB2mCxzytTPCYbwPOkfQ4UAGuiIitnYv64EzwmD8BfFnSH1AbIvnAVP5gJ+kmakN7c7N5j6uAHoCI+BK1eZDzgXXAbuC3D3qfU/jnZWZmk2A6Dg2Zmdl+cCEwM0ucC4GZWeJcCMzMEudCYGaWOBcCswmSVJG0uuFriaQzJW3P7ny5RtJVWdvG9U9I+lyn4zcby7S7jsAsRy9ExCmNK7JbmN8dEb8paSawWlL93kb19YcAD0v6x4i4p70hm+2bewRmkyQidgEPAi9rWv8CsJqDvDGYWV5cCMwm7pCGYaF/bN4o6Shq9zR6rGn9EdTu93NXe8I02z8eGjKbuFFDQ5k3SnoYqAL/K7sFwpnZ+keBE7L1/9HGWM0mzIXA7ODdHRG/OdZ6SccDP8nmCFa3OzizffHQkFnOIuLfgU8Df9TpWMxacSEwa48vAW+StLTTgZg1891HzcwS5x6BmVniXAjMzBLnQmBmljgXAjOzxLkQmJklzoXAzCxxLgRmZon7/weTYgwb7UJOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "thresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "tpr_points = []\n",
    "fpr_points = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_spam_pred_t = []\n",
    "    for prob in prob_results_spam:\n",
    "        if (prob[1] > t):\n",
    "            y_spam_pred_t.append(1)\n",
    "        else:\n",
    "            y_spam_pred_t.append(0)\n",
    "            \n",
    "    cm_spam_t = metrics.confusion_matrix(y_spam_test, y_spam_pred_t)\n",
    "    tpr_points.append(cm_spam_t[0][0] / (cm_spam_t[0][0] + cm_spam_t[0][1]))\n",
    "    fpr_points.append(cm_spam_t[1][0] / (cm_spam_t[1][0] + cm_spam_t[1][1]))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel('FPR')\n",
    "ax.set_ylabel('TPR')\n",
    "ax.set_title('ROC Curve')\n",
    "ax.plot(fpr_points, tpr_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The diffrenences are mostly in the smoothness of the curve, in which mine is smoother given that it has far less points. The way to make it more similar would be to add more points for each specific threshold (eg. T = 0, T =.00000001, ... T = 1).\n",
    "\n",
    "# Problem 4\n",
    "## 4a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_k(k, model, X, y):\n",
    "    partition_size = len(y.values) / k\n",
    "    error = 0\n",
    "    for idx in range(k):\n",
    "        X_train = X.copy()\n",
    "        X_test = pd.DataFrame(columns=columns_spam)\n",
    "        y_train = y.copy()\n",
    "        y_test = pd.DataFrame(columns=['class'])\n",
    "        for jdx in range(len(y.values)):\n",
    "            if (jdx >= idx * partition_size and jdx < (idx + 1) * partition_size):\n",
    "                df_x_temp = pd.DataFrame([X.iloc[jdx].values], columns=columns_spam, index=[jdx])\n",
    "                X_test = pd.concat([X_test, df_x_temp], axis=0)\n",
    "                X_train = X_train.drop([jdx])\n",
    "                df_y_temp = pd.DataFrame([y.iloc[jdx]], columns=['class'], index=[jdx])\n",
    "                y_test = pd.concat([y_test, df_y_temp], axis=0)\n",
    "                y_train = y_train.drop([jdx])\n",
    "        if model:\n",
    "            clf = LogisticRegression()\n",
    "        else:\n",
    "            clf = LinearDiscriminantAnalysis()\n",
    "        clf.fit(X_train,y_train)\n",
    "        pred = clf.predict(X_test)\n",
    "        mistakes = 0\n",
    "        for jdx in range(len(y_test.values)):\n",
    "            if(y_test.values[idx][0] != pred[idx]):\n",
    "                mistakes += 1\n",
    "        error += mistakes / len(y_test.values)\n",
    "    error /= k\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for Logistic Regression with k=5is 0.4\n",
      "Error for LDA with k=5is 0.4\n",
      "Error for Logistic Regression with k=10is 0.2\n",
      "Error for LDA with k=10is 0.3\n"
     ]
    }
   ],
   "source": [
    "k = [5, 10]\n",
    "model = [True, False]\n",
    "model_name = [\"Logistic Regression\", \"LDA\"]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        print(\"Error for \" + model_name[j] + \" with k=\" + str(k[i]) + \" is \" + str(CV_k(k[i], model[j], X_spam, y_spam)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_k(5, True, X_spam, y_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c)\n",
    "\n",
    "#### Logistic Regression seems to perform marginally better. Again, this is likely due to LDA being generative with only two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
